GENERAL INFO ABOUT THE DATASET
The preprocessed dataset is in the form (n timesteps, n features), where the features are in the order (datetimeIndex, t outdoor, q cool, q heat, t indoor). 

HOW TO USE THE DATASET
1. The Dataset used by the model is in the directory below:

    Data/lbnlbldg59/processed/shuffled_data/multi_feature.pkl

2. Dataset has to be reshaped using the following function:

    dataset = np.reshape(dataset, (int(len(dataset)/48), 48, 5)),

  where 48 is the number of observations per day (frequency is 30 min).

3. Dataset has to be devided into training set, validation set and evaluation set by using the following function:

    trX_dataset, teX_dataset, ttX_dataset = np.split(dataset, [int(train_rate * len(dataset)),int((train_rate + 0.1) * len(dataset))]),

   where train_rate is set to 0.025.

4. Drop the first 2 features as we are not interested in corrupting "datetimeIndex" and "t outdoor"

5. The index for corrupted values has to be generated by using the following function:

    def corruption(input,corr,missing):
        """ corruption function """
        mask_noisy = np.zeros(shape=(input.shape[0], input.shape[1], input.shape[2]))
        if missing == 'continuous':
            length = np.int(np.round(corr * input.shape[1]))
            for row in range(input.shape[0]):
                start = np.random.randint(2, input.shape[1] - length - 2)
                mask_noisy[row, start:start + length, :] = np.nan
            return np.where(np.isnan(mask_noisy)), np.where(np.isfinite(mask_noisy))
        elif missing == 'random':
            for row in range(input.shape[0]):
                indices = np.random.choice(np.arange(2,input.shape[1]-2), replace=False,size=np.int(np.round(input.shape[1] * corr)))
                mask_noisy[row,indices, :] = np.nan
            return np.where(np.isnan(mask_noisy)), np.where(np.isfinite(mask_noisy))
        else:
            print("error: missing")

    args:
         "input": is the used training, validation or evaluation set
         "corr": is the used corruption rate, i.e. {0.2, 0.4, 0.6, 0.8}
         "missing": defines if "continuous" or "random" missing

    returns:
             "np.where(np.isnan(mask_noisy))": the index for missing data
             "np.where(np.isfinite(mask_noisy))": the index for NON missing data

6. The corrupted training, validation and evaluation sets can be obtained by using the following function:
   
    input[np.where(np.isnan(mask_noisy))] = 0 (or NaN according to how your model is working)

   to account for the randomness of the corruption processs evaluation is run 100 times and final results are averaged.

7. Evaluation:
    
    used metrics: mae, rmse.

    Evaluate the metrics on each single day of the evaluation set and take the average to show final performance.
    


